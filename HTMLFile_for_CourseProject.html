<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Prediction Study Design - Alexandre Xavier Ywata de Carvalho</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<p>Course: Practical Machine Learning</p>

<h1>Prediction Study Design - Alexandre Xavier Ywata de Carvalho</h1>

<ol>
<li>Data Preparation for the Prediction Study Design</li>
</ol>

<p>To predict the outcome variable, I first imported the training data and the testing data. I extracted some summary statistics and noticed that, although the table is quite complex, in terms of possible predictive variables, some of these variables had several rows with missing data. I then decided to keep and use only the variables with no missing data for all rows. All the operations of columns extraction was done for both training and testing data.</p>

<ol>
<li>Selecting Training, Testing and Evaluation Samples</li>
</ol>

<p>I then focused only on the training data set. For this dataset, I separated the data into three subsets. The first dataset was used for training only; the second dataset was used for testing. The third one was used for validation of the final model. The training dataset had 60% of the observations; the testing and the validation datasets had each one of them 20% the observations in the original imported training table. Therefore, I had the mix 60%, 20%, 20% for the size of the training, testing and validation datasets. I reinforce that I am using now only the predicting variables with no missing data.</p>

<ol>
<li>Choosing the Model</li>
</ol>

<p>For the training sub dataset (60% of the original training imported dataset), I initially applied a principal component analysis to extract components explaining up to 95% of the total linear variation on the valid predictors (predictors with no missing data). Specifying the rotation for the principal components was done using exclusively the training sub dataset. I then applied the same rotation to the testing sub dataset (20% of the original training imported dataset). Using the training sub dataset for training and the testing sub dataset for evaluating the out-of-sample prediction error, I then tried several different models. When possible, I tried models with principal components compression or without it. The models tried were:</p>

<pre><code>1.  Linear discriminant analysis
2.  Linear discriminant analysis to the principal components 
3.  Classification trees
4.  Classification trees to the principal components
5.  Boosting 
6.  Boosting to the principal components
7.  Random forests to the principal components
8.  Bagging with LDA for the principal components
9.  Regularized discriminant analysis
</code></pre>

<p>Models 5, 7, 8 and 9 were not possible to train in the end, due to computer memory issues. I emphasize that each model was trained using only the training sub dataset (60% of the original training imported dataset). I used the testing sub dataset (20% of the original training imported dataset) to calculated out-of-sample prediction errors, for each model, and I found out that the best model, in terms of out-of-sample prediction, was the boosting applied to the principal components. </p>

<ol>
<li>Refining the Model</li>
</ol>

<p>After selecting the best model, using 60% of the original sample for training, and 20% of the original sample for testing the out-of-sample performance for each tried model, I re-estimated the best model (boosting with PCA) to the 80% sub dataset, consisting of the training sub dataset plus the testing sub dataset. For this new trained model, I evaluated the out-of-sample performance using the validation sub dataset (the other 20% of the original sample, left out of the new training). The performance of the model in the validation sub dataset was very good: I obtained an accuracy of 83.43%. I was very careful in applying the principal components rotation, based only on the data used for training the model.</p>

<ol>
<li>Using the Final Model</li>
</ol>

<p>Finally, the last step would be to use the selected model (boosting with PCA) and trained it one more time, now using the whole training original data. In that way, I would be able to use all information in the provided training database. However, when trying to train this new model (with 100% of the original training data), I had again computer memory issues. I then decided to use the previous model, estimated with 80% of the original provided training sample. </p>

<p>I then applied the best available trained model to the provided testing dataset. Once again, I was very careful in applying the principal components rotation, based only on the data effectively used for training the model. The final predicted categories were:</p>

<pre><code>  Observation   Predicted Classe
  1             B
  2             A
  3             C
  4             A
  5             A
  6             E
  7             E
  8             D
  9             A
  10              A
  11              A
  12              C
  13              B
  14              A
  15              E
  16              E
  17              A
  18              B
  19              A
  20              B
</code></pre>

<pre><code class="r">rm(list=ls());

set.seed(2104);

library(caret);
</code></pre>

<pre><code>## Warning: package &#39;caret&#39; was built under R version 3.0.3
</code></pre>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.0.3
</code></pre>

<pre><code class="r">library(ggplot2);

#reading the data

estimdata &lt;- read.csv(&quot;C:/Alex/AulasWEB/PracticalMachineLearning/R_programs/pml-training.csv&quot;);
testdata &lt;- read.csv(&quot;C:/Alex/AulasWEB/PracticalMachineLearning/R_programs/pml-testing.csv&quot;);

estimdata &lt;- as.data.frame(estimdata);
testdata &lt;- as.data.frame(testdata);

dim(estimdata);
</code></pre>

<pre><code>## [1] 19622   160
</code></pre>

<pre><code class="r"># names(estimdata);

# names(testdata);
dim(testdata);
</code></pre>

<pre><code>## [1]  20 160
</code></pre>

<pre><code class="r">table(estimdata$classe);
</code></pre>

<pre><code>## 
##    A    B    C    D    E 
## 5580 3797 3422 3216 3607
</code></pre>

<pre><code class="r">is.factor(estimdata$classe);
</code></pre>

<pre><code>## [1] TRUE
</code></pre>

<pre><code class="r">estimdata &lt;- subset(estimdata, select=-c(X, user_name, new_window));
names(estimdata);
</code></pre>

<pre><code>##   [1] &quot;raw_timestamp_part_1&quot;     &quot;raw_timestamp_part_2&quot;    
##   [3] &quot;cvtd_timestamp&quot;           &quot;num_window&quot;              
##   [5] &quot;roll_belt&quot;                &quot;pitch_belt&quot;              
##   [7] &quot;yaw_belt&quot;                 &quot;total_accel_belt&quot;        
##   [9] &quot;kurtosis_roll_belt&quot;       &quot;kurtosis_picth_belt&quot;     
##  [11] &quot;kurtosis_yaw_belt&quot;        &quot;skewness_roll_belt&quot;      
##  [13] &quot;skewness_roll_belt.1&quot;     &quot;skewness_yaw_belt&quot;       
##  [15] &quot;max_roll_belt&quot;            &quot;max_picth_belt&quot;          
##  [17] &quot;max_yaw_belt&quot;             &quot;min_roll_belt&quot;           
##  [19] &quot;min_pitch_belt&quot;           &quot;min_yaw_belt&quot;            
##  [21] &quot;amplitude_roll_belt&quot;      &quot;amplitude_pitch_belt&quot;    
##  [23] &quot;amplitude_yaw_belt&quot;       &quot;var_total_accel_belt&quot;    
##  [25] &quot;avg_roll_belt&quot;            &quot;stddev_roll_belt&quot;        
##  [27] &quot;var_roll_belt&quot;            &quot;avg_pitch_belt&quot;          
##  [29] &quot;stddev_pitch_belt&quot;        &quot;var_pitch_belt&quot;          
##  [31] &quot;avg_yaw_belt&quot;             &quot;stddev_yaw_belt&quot;         
##  [33] &quot;var_yaw_belt&quot;             &quot;gyros_belt_x&quot;            
##  [35] &quot;gyros_belt_y&quot;             &quot;gyros_belt_z&quot;            
##  [37] &quot;accel_belt_x&quot;             &quot;accel_belt_y&quot;            
##  [39] &quot;accel_belt_z&quot;             &quot;magnet_belt_x&quot;           
##  [41] &quot;magnet_belt_y&quot;            &quot;magnet_belt_z&quot;           
##  [43] &quot;roll_arm&quot;                 &quot;pitch_arm&quot;               
##  [45] &quot;yaw_arm&quot;                  &quot;total_accel_arm&quot;         
##  [47] &quot;var_accel_arm&quot;            &quot;avg_roll_arm&quot;            
##  [49] &quot;stddev_roll_arm&quot;          &quot;var_roll_arm&quot;            
##  [51] &quot;avg_pitch_arm&quot;            &quot;stddev_pitch_arm&quot;        
##  [53] &quot;var_pitch_arm&quot;            &quot;avg_yaw_arm&quot;             
##  [55] &quot;stddev_yaw_arm&quot;           &quot;var_yaw_arm&quot;             
##  [57] &quot;gyros_arm_x&quot;              &quot;gyros_arm_y&quot;             
##  [59] &quot;gyros_arm_z&quot;              &quot;accel_arm_x&quot;             
##  [61] &quot;accel_arm_y&quot;              &quot;accel_arm_z&quot;             
##  [63] &quot;magnet_arm_x&quot;             &quot;magnet_arm_y&quot;            
##  [65] &quot;magnet_arm_z&quot;             &quot;kurtosis_roll_arm&quot;       
##  [67] &quot;kurtosis_picth_arm&quot;       &quot;kurtosis_yaw_arm&quot;        
##  [69] &quot;skewness_roll_arm&quot;        &quot;skewness_pitch_arm&quot;      
##  [71] &quot;skewness_yaw_arm&quot;         &quot;max_roll_arm&quot;            
##  [73] &quot;max_picth_arm&quot;            &quot;max_yaw_arm&quot;             
##  [75] &quot;min_roll_arm&quot;             &quot;min_pitch_arm&quot;           
##  [77] &quot;min_yaw_arm&quot;              &quot;amplitude_roll_arm&quot;      
##  [79] &quot;amplitude_pitch_arm&quot;      &quot;amplitude_yaw_arm&quot;       
##  [81] &quot;roll_dumbbell&quot;            &quot;pitch_dumbbell&quot;          
##  [83] &quot;yaw_dumbbell&quot;             &quot;kurtosis_roll_dumbbell&quot;  
##  [85] &quot;kurtosis_picth_dumbbell&quot;  &quot;kurtosis_yaw_dumbbell&quot;   
##  [87] &quot;skewness_roll_dumbbell&quot;   &quot;skewness_pitch_dumbbell&quot; 
##  [89] &quot;skewness_yaw_dumbbell&quot;    &quot;max_roll_dumbbell&quot;       
##  [91] &quot;max_picth_dumbbell&quot;       &quot;max_yaw_dumbbell&quot;        
##  [93] &quot;min_roll_dumbbell&quot;        &quot;min_pitch_dumbbell&quot;      
##  [95] &quot;min_yaw_dumbbell&quot;         &quot;amplitude_roll_dumbbell&quot; 
##  [97] &quot;amplitude_pitch_dumbbell&quot; &quot;amplitude_yaw_dumbbell&quot;  
##  [99] &quot;total_accel_dumbbell&quot;     &quot;var_accel_dumbbell&quot;      
## [101] &quot;avg_roll_dumbbell&quot;        &quot;stddev_roll_dumbbell&quot;    
## [103] &quot;var_roll_dumbbell&quot;        &quot;avg_pitch_dumbbell&quot;      
## [105] &quot;stddev_pitch_dumbbell&quot;    &quot;var_pitch_dumbbell&quot;      
## [107] &quot;avg_yaw_dumbbell&quot;         &quot;stddev_yaw_dumbbell&quot;     
## [109] &quot;var_yaw_dumbbell&quot;         &quot;gyros_dumbbell_x&quot;        
## [111] &quot;gyros_dumbbell_y&quot;         &quot;gyros_dumbbell_z&quot;        
## [113] &quot;accel_dumbbell_x&quot;         &quot;accel_dumbbell_y&quot;        
## [115] &quot;accel_dumbbell_z&quot;         &quot;magnet_dumbbell_x&quot;       
## [117] &quot;magnet_dumbbell_y&quot;        &quot;magnet_dumbbell_z&quot;       
## [119] &quot;roll_forearm&quot;             &quot;pitch_forearm&quot;           
## [121] &quot;yaw_forearm&quot;              &quot;kurtosis_roll_forearm&quot;   
## [123] &quot;kurtosis_picth_forearm&quot;   &quot;kurtosis_yaw_forearm&quot;    
## [125] &quot;skewness_roll_forearm&quot;    &quot;skewness_pitch_forearm&quot;  
## [127] &quot;skewness_yaw_forearm&quot;     &quot;max_roll_forearm&quot;        
## [129] &quot;max_picth_forearm&quot;        &quot;max_yaw_forearm&quot;         
## [131] &quot;min_roll_forearm&quot;         &quot;min_pitch_forearm&quot;       
## [133] &quot;min_yaw_forearm&quot;          &quot;amplitude_roll_forearm&quot;  
## [135] &quot;amplitude_pitch_forearm&quot;  &quot;amplitude_yaw_forearm&quot;   
## [137] &quot;total_accel_forearm&quot;      &quot;var_accel_forearm&quot;       
## [139] &quot;avg_roll_forearm&quot;         &quot;stddev_roll_forearm&quot;     
## [141] &quot;var_roll_forearm&quot;         &quot;avg_pitch_forearm&quot;       
## [143] &quot;stddev_pitch_forearm&quot;     &quot;var_pitch_forearm&quot;       
## [145] &quot;avg_yaw_forearm&quot;          &quot;stddev_yaw_forearm&quot;      
## [147] &quot;var_yaw_forearm&quot;          &quot;gyros_forearm_x&quot;         
## [149] &quot;gyros_forearm_y&quot;          &quot;gyros_forearm_z&quot;         
## [151] &quot;accel_forearm_x&quot;          &quot;accel_forearm_y&quot;         
## [153] &quot;accel_forearm_z&quot;          &quot;magnet_forearm_x&quot;        
## [155] &quot;magnet_forearm_y&quot;         &quot;magnet_forearm_z&quot;        
## [157] &quot;classe&quot;
</code></pre>

<pre><code class="r">testdata &lt;- subset(testdata, select=-c(X, user_name, new_window));
names(testdata);
</code></pre>

<pre><code>##   [1] &quot;raw_timestamp_part_1&quot;     &quot;raw_timestamp_part_2&quot;    
##   [3] &quot;cvtd_timestamp&quot;           &quot;num_window&quot;              
##   [5] &quot;roll_belt&quot;                &quot;pitch_belt&quot;              
##   [7] &quot;yaw_belt&quot;                 &quot;total_accel_belt&quot;        
##   [9] &quot;kurtosis_roll_belt&quot;       &quot;kurtosis_picth_belt&quot;     
##  [11] &quot;kurtosis_yaw_belt&quot;        &quot;skewness_roll_belt&quot;      
##  [13] &quot;skewness_roll_belt.1&quot;     &quot;skewness_yaw_belt&quot;       
##  [15] &quot;max_roll_belt&quot;            &quot;max_picth_belt&quot;          
##  [17] &quot;max_yaw_belt&quot;             &quot;min_roll_belt&quot;           
##  [19] &quot;min_pitch_belt&quot;           &quot;min_yaw_belt&quot;            
##  [21] &quot;amplitude_roll_belt&quot;      &quot;amplitude_pitch_belt&quot;    
##  [23] &quot;amplitude_yaw_belt&quot;       &quot;var_total_accel_belt&quot;    
##  [25] &quot;avg_roll_belt&quot;            &quot;stddev_roll_belt&quot;        
##  [27] &quot;var_roll_belt&quot;            &quot;avg_pitch_belt&quot;          
##  [29] &quot;stddev_pitch_belt&quot;        &quot;var_pitch_belt&quot;          
##  [31] &quot;avg_yaw_belt&quot;             &quot;stddev_yaw_belt&quot;         
##  [33] &quot;var_yaw_belt&quot;             &quot;gyros_belt_x&quot;            
##  [35] &quot;gyros_belt_y&quot;             &quot;gyros_belt_z&quot;            
##  [37] &quot;accel_belt_x&quot;             &quot;accel_belt_y&quot;            
##  [39] &quot;accel_belt_z&quot;             &quot;magnet_belt_x&quot;           
##  [41] &quot;magnet_belt_y&quot;            &quot;magnet_belt_z&quot;           
##  [43] &quot;roll_arm&quot;                 &quot;pitch_arm&quot;               
##  [45] &quot;yaw_arm&quot;                  &quot;total_accel_arm&quot;         
##  [47] &quot;var_accel_arm&quot;            &quot;avg_roll_arm&quot;            
##  [49] &quot;stddev_roll_arm&quot;          &quot;var_roll_arm&quot;            
##  [51] &quot;avg_pitch_arm&quot;            &quot;stddev_pitch_arm&quot;        
##  [53] &quot;var_pitch_arm&quot;            &quot;avg_yaw_arm&quot;             
##  [55] &quot;stddev_yaw_arm&quot;           &quot;var_yaw_arm&quot;             
##  [57] &quot;gyros_arm_x&quot;              &quot;gyros_arm_y&quot;             
##  [59] &quot;gyros_arm_z&quot;              &quot;accel_arm_x&quot;             
##  [61] &quot;accel_arm_y&quot;              &quot;accel_arm_z&quot;             
##  [63] &quot;magnet_arm_x&quot;             &quot;magnet_arm_y&quot;            
##  [65] &quot;magnet_arm_z&quot;             &quot;kurtosis_roll_arm&quot;       
##  [67] &quot;kurtosis_picth_arm&quot;       &quot;kurtosis_yaw_arm&quot;        
##  [69] &quot;skewness_roll_arm&quot;        &quot;skewness_pitch_arm&quot;      
##  [71] &quot;skewness_yaw_arm&quot;         &quot;max_roll_arm&quot;            
##  [73] &quot;max_picth_arm&quot;            &quot;max_yaw_arm&quot;             
##  [75] &quot;min_roll_arm&quot;             &quot;min_pitch_arm&quot;           
##  [77] &quot;min_yaw_arm&quot;              &quot;amplitude_roll_arm&quot;      
##  [79] &quot;amplitude_pitch_arm&quot;      &quot;amplitude_yaw_arm&quot;       
##  [81] &quot;roll_dumbbell&quot;            &quot;pitch_dumbbell&quot;          
##  [83] &quot;yaw_dumbbell&quot;             &quot;kurtosis_roll_dumbbell&quot;  
##  [85] &quot;kurtosis_picth_dumbbell&quot;  &quot;kurtosis_yaw_dumbbell&quot;   
##  [87] &quot;skewness_roll_dumbbell&quot;   &quot;skewness_pitch_dumbbell&quot; 
##  [89] &quot;skewness_yaw_dumbbell&quot;    &quot;max_roll_dumbbell&quot;       
##  [91] &quot;max_picth_dumbbell&quot;       &quot;max_yaw_dumbbell&quot;        
##  [93] &quot;min_roll_dumbbell&quot;        &quot;min_pitch_dumbbell&quot;      
##  [95] &quot;min_yaw_dumbbell&quot;         &quot;amplitude_roll_dumbbell&quot; 
##  [97] &quot;amplitude_pitch_dumbbell&quot; &quot;amplitude_yaw_dumbbell&quot;  
##  [99] &quot;total_accel_dumbbell&quot;     &quot;var_accel_dumbbell&quot;      
## [101] &quot;avg_roll_dumbbell&quot;        &quot;stddev_roll_dumbbell&quot;    
## [103] &quot;var_roll_dumbbell&quot;        &quot;avg_pitch_dumbbell&quot;      
## [105] &quot;stddev_pitch_dumbbell&quot;    &quot;var_pitch_dumbbell&quot;      
## [107] &quot;avg_yaw_dumbbell&quot;         &quot;stddev_yaw_dumbbell&quot;     
## [109] &quot;var_yaw_dumbbell&quot;         &quot;gyros_dumbbell_x&quot;        
## [111] &quot;gyros_dumbbell_y&quot;         &quot;gyros_dumbbell_z&quot;        
## [113] &quot;accel_dumbbell_x&quot;         &quot;accel_dumbbell_y&quot;        
## [115] &quot;accel_dumbbell_z&quot;         &quot;magnet_dumbbell_x&quot;       
## [117] &quot;magnet_dumbbell_y&quot;        &quot;magnet_dumbbell_z&quot;       
## [119] &quot;roll_forearm&quot;             &quot;pitch_forearm&quot;           
## [121] &quot;yaw_forearm&quot;              &quot;kurtosis_roll_forearm&quot;   
## [123] &quot;kurtosis_picth_forearm&quot;   &quot;kurtosis_yaw_forearm&quot;    
## [125] &quot;skewness_roll_forearm&quot;    &quot;skewness_pitch_forearm&quot;  
## [127] &quot;skewness_yaw_forearm&quot;     &quot;max_roll_forearm&quot;        
## [129] &quot;max_picth_forearm&quot;        &quot;max_yaw_forearm&quot;         
## [131] &quot;min_roll_forearm&quot;         &quot;min_pitch_forearm&quot;       
## [133] &quot;min_yaw_forearm&quot;          &quot;amplitude_roll_forearm&quot;  
## [135] &quot;amplitude_pitch_forearm&quot;  &quot;amplitude_yaw_forearm&quot;   
## [137] &quot;total_accel_forearm&quot;      &quot;var_accel_forearm&quot;       
## [139] &quot;avg_roll_forearm&quot;         &quot;stddev_roll_forearm&quot;     
## [141] &quot;var_roll_forearm&quot;         &quot;avg_pitch_forearm&quot;       
## [143] &quot;stddev_pitch_forearm&quot;     &quot;var_pitch_forearm&quot;       
## [145] &quot;avg_yaw_forearm&quot;          &quot;stddev_yaw_forearm&quot;      
## [147] &quot;var_yaw_forearm&quot;          &quot;gyros_forearm_x&quot;         
## [149] &quot;gyros_forearm_y&quot;          &quot;gyros_forearm_z&quot;         
## [151] &quot;accel_forearm_x&quot;          &quot;accel_forearm_y&quot;         
## [153] &quot;accel_forearm_z&quot;          &quot;magnet_forearm_x&quot;        
## [155] &quot;magnet_forearm_y&quot;         &quot;magnet_forearm_z&quot;        
## [157] &quot;problem_id&quot;
</code></pre>

<pre><code class="r">#excluding variables with missing data 

indicator_numeric_columns &lt;- sapply(estimdata, is.numeric);

estimdata_num &lt;- estimdata[,indicator_numeric_columns];

names_num_columns_with_na &lt;- names(estimdata_num[is.na(colMeans(estimdata_num))]);

estimdata_valid &lt;- estimdata_num[,!(names(estimdata_num) %in% names_num_columns_with_na)];

testdata_num &lt;- testdata[, indicator_numeric_columns];
testdata_valid &lt;- testdata_num[,!(names(testdata_num) %in% names_num_columns_with_na)];

classe = estimdata$classe;
estimdata_valid = data.frame(classe, estimdata_valid);

dim(estimdata_valid); dim(testdata_valid);
</code></pre>

<pre><code>## [1] 19622    56
</code></pre>

<pre><code>## [1] 20 55
</code></pre>

<pre><code class="r">#creating samples for training, testing and evaluating the final model selected (60%, 20% and 20% in each sample)

inTrain &lt;- createDataPartition(y = estimdata_valid$classe, p=0.80, list=FALSE);

training0 &lt;- estimdata_valid[inTrain,];
evaluating &lt;- estimdata_valid[-inTrain,];

inTrain1 &lt;- createDataPartition(y = training0$classe, p = 0.75, list=FALSE);

training &lt;- training0[inTrain1,];
testing &lt;- training0[-inTrain1,];

dim(training); dim(testing); dim(evaluating); dim(estimdata_valid);
</code></pre>

<pre><code>## [1] 11776    56
</code></pre>

<pre><code>## [1] 3923   56
</code></pre>

<pre><code>## [1] 3923   56
</code></pre>

<pre><code>## [1] 19622    56
</code></pre>

<pre><code class="r">teste_dimensions &lt;- dim(training)[1] + dim(testing)[1] + dim(evaluating)[1] - dim(estimdata_valid)[1];
percentagem_training &lt;- dim(training)[1] / dim(estimdata_valid)[1];
percentagem_testing &lt;- dim(testing)[1] / dim(estimdata_valid)[1];
percentagem_evaluating &lt;- dim(evaluating)[1] / dim(estimdata_valid)[1];

teste_dimensions
</code></pre>

<pre><code>## [1] 0
</code></pre>

<pre><code class="r">percentagem_training
</code></pre>

<pre><code>## [1] 0.6001
</code></pre>

<pre><code class="r">percentagem_testing
</code></pre>

<pre><code>## [1] 0.1999
</code></pre>

<pre><code class="r">percentagem_evaluating
</code></pre>

<pre><code>## [1] 0.1999
</code></pre>

<pre><code class="r">#creating principal components for data compression

preObj &lt;- preProcess(training[,-1], method = &quot;pca&quot;, thresh=0.95);
preObj
</code></pre>

<pre><code>## 
## Call:
## preProcess.default(x = training[, -1], method = &quot;pca&quot;, thresh = 0.95)
## 
## Created from 11776 samples and 55 variables
## Pre-processing: principal component signal extraction, scaled, centered 
## 
## PCA needed 28 components to capture 95 percent of the variance
</code></pre>

<pre><code class="r">training_pca &lt;- data.frame(classe = training$classe, predict(preObj, training[,-1]));
testing_pca &lt;- data.frame(classe = testing$classe, predict(preObj, testing[,-1]));

#linear discriminant analysis

set.seed(2104);
modFit1 &lt;- train(classe ~ ., method=&quot;lda&quot;, data=training);
</code></pre>

<pre><code>## Loading required package: MASS
</code></pre>

<pre><code>## Warning: package &#39;e1071&#39; was built under R version 3.0.3
</code></pre>

<pre><code class="r">preds &lt;- predict(modFit1, newdata=testing);
confusionMatrix(preds, testing$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 946 121  73  35  46
##          B  32 494  59  29 100
##          C  54  90 442  80  50
##          D  79  30  89 478  77
##          E   5  24  21  21 448
## 
## Overall Statistics
##                                        
##                Accuracy : 0.716        
##                  95% CI : (0.701, 0.73)
##     No Information Rate : 0.284        
##     P-Value [Acc &gt; NIR] : &lt;2e-16       
##                                        
##                   Kappa : 0.64         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.848    0.651    0.646    0.743    0.621
## Specificity             0.902    0.930    0.915    0.916    0.978
## Pos Pred Value          0.775    0.692    0.617    0.635    0.863
## Neg Pred Value          0.937    0.917    0.925    0.948    0.920
## Prevalence              0.284    0.193    0.174    0.164    0.184
## Detection Rate          0.241    0.126    0.113    0.122    0.114
## Detection Prevalence    0.311    0.182    0.183    0.192    0.132
## Balanced Accuracy       0.875    0.791    0.781    0.830    0.800
</code></pre>

<pre><code class="r">#linear discriminant analysis com pca

set.seed(2104);
modFit2 &lt;- train(classe ~ ., method=&quot;lda&quot;, data=training_pca);
preds &lt;- predict(modFit2, newdata=testing_pca);
confusionMatrix(preds, testing_pca$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 736 172 180  39  79
##          B  96 318  60  72 126
##          C 105 141 355 120  91
##          D 126  89  58 340  85
##          E  53  39  31  72 340
## 
## Overall Statistics
##                                         
##                Accuracy : 0.533         
##                  95% CI : (0.517, 0.548)
##     No Information Rate : 0.284         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.408         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.659   0.4190   0.5190   0.5288   0.4716
## Specificity             0.833   0.8881   0.8589   0.8909   0.9391
## Pos Pred Value          0.610   0.4732   0.4372   0.4871   0.6355
## Neg Pred Value          0.860   0.8643   0.8942   0.9060   0.8875
## Prevalence              0.284   0.1935   0.1744   0.1639   0.1838
## Detection Rate          0.188   0.0811   0.0905   0.0867   0.0867
## Detection Prevalence    0.307   0.1713   0.2070   0.1779   0.1364
## Balanced Accuracy       0.746   0.6535   0.6890   0.7098   0.7053
</code></pre>

<pre><code class="r">#classification trees

set.seed(2104);
modFit3 &lt;- train(classe ~ ., method=&quot;rpart&quot;, data=training);
</code></pre>

<pre><code>## Loading required package: rpart
</code></pre>

<pre><code class="r">preds &lt;- predict(modFit3, newdata=testing);
confusionMatrix(preds, testing$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1011  322  320  266   70
##          B   14  244   23  102   53
##          C   86  193  341  240  146
##          D    0    0    0    0    0
##          E    5    0    0   35  452
## 
## Overall Statistics
##                                         
##                Accuracy : 0.522         
##                  95% CI : (0.506, 0.538)
##     No Information Rate : 0.284         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.376         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.906   0.3215   0.4985    0.000    0.627
## Specificity             0.652   0.9393   0.7947    1.000    0.988
## Pos Pred Value          0.508   0.5596   0.3390      NaN    0.919
## Neg Pred Value          0.946   0.8523   0.8824    0.836    0.922
## Prevalence              0.284   0.1935   0.1744    0.164    0.184
## Detection Rate          0.258   0.0622   0.0869    0.000    0.115
## Detection Prevalence    0.507   0.1111   0.2564    0.000    0.125
## Balanced Accuracy       0.779   0.6304   0.6466    0.500    0.807
</code></pre>

<pre><code class="r">#classification trees com pca

set.seed(2104);
modFit4 &lt;- train(classe ~ ., method=&quot;rpart&quot;, data=training_pca);
preds &lt;- predict(modFit4, newdata=testing_pca);
confusionMatrix(preds, testing_pca$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 740 170 278 154 151
##          B 148 293  75  69 116
##          C 136 121 261 108  68
##          D  65 122  63 212  80
##          E  27  53   7 100 306
## 
## Overall Statistics
##                                         
##                Accuracy : 0.462         
##                  95% CI : (0.446, 0.478)
##     No Information Rate : 0.284         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.311         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.663   0.3860   0.3816    0.330    0.424
## Specificity             0.732   0.8710   0.8663    0.899    0.942
## Pos Pred Value          0.496   0.4180   0.3761    0.391    0.621
## Neg Pred Value          0.845   0.8554   0.8690    0.873    0.879
## Prevalence              0.284   0.1935   0.1744    0.164    0.184
## Detection Rate          0.189   0.0747   0.0665    0.054    0.078
## Detection Prevalence    0.381   0.1787   0.1769    0.138    0.126
## Balanced Accuracy       0.697   0.6285   0.6239    0.615    0.683
</code></pre>

<pre><code class="r">#boosting 

#set.seed(2104);
#modFit5 &lt;- train(classe ~ ., method=&quot;gbm&quot;, data=training, verbose=FALSE);
#preds &lt;- predict(modFit5, newdata=testing);
#confusionMatrix(preds, testing$classe)

#boosting com pca

set.seed(2104);
modFit6 &lt;- train(classe ~ ., method=&quot;gbm&quot;, data=training_pca, verbose=FALSE);
</code></pre>

<pre><code>## Loading required package: gbm
</code></pre>

<pre><code>## Warning: package &#39;gbm&#39; was built under R version 3.0.3
</code></pre>

<pre><code>## Loading required package: survival
## Loading required package: splines
## 
## Attaching package: &#39;survival&#39;
## 
## The following object is masked from &#39;package:caret&#39;:
## 
##     cluster
## 
## Loading required package: parallel
## Loaded gbm 2.1
## Loading required package: plyr
</code></pre>

<pre><code class="r">preds &lt;- predict(modFit6, newdata=testing_pca);
confusionMatrix(preds, testing_pca$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 996  94  49  20  34
##          B  33 586  49  21  41
##          C  30  36 562  80  30
##          D  48  23  16 498  41
##          E   9  20   8  24 575
## 
## Overall Statistics
##                                         
##                Accuracy : 0.82          
##                  95% CI : (0.808, 0.832)
##     No Information Rate : 0.284         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.772         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.892    0.772    0.822    0.774    0.798
## Specificity             0.930    0.954    0.946    0.961    0.981
## Pos Pred Value          0.835    0.803    0.762    0.796    0.904
## Neg Pred Value          0.956    0.946    0.962    0.956    0.956
## Prevalence              0.284    0.193    0.174    0.164    0.184
## Detection Rate          0.254    0.149    0.143    0.127    0.147
## Detection Prevalence    0.304    0.186    0.188    0.160    0.162
## Balanced Accuracy       0.911    0.863    0.884    0.868    0.889
</code></pre>

<pre><code class="r">#random forests com pca

#set.seed(2104);
#modFit6 &lt;- train(classe ~ ., method=&quot;rf&quot;, data=training_pca, prox=TRUE);
#preds &lt;- predict(modFit6, newdata=testing_pca);
#confusionMatrix(preds, testing_pca$classe)

#bagging with lda com pca

#set.seed(2104);
#modFit7 &lt;- train(classe ~ .,  
#                method = &quot;bag&quot;, 
#                B = 10, 
#                data = training_pca,
#                bagControl = bagControl(fit = ldaBag$fit,
#                                        predict = ldaBag$pred,
#                                        aggregate = ldaBag$aggregate),
#                tuneGrid = data.frame(vars = c((1:10)*10 , ncol(training_pca))));
#preds &lt;- predict(modFit7, newdata=testing_pca);
#confusionMatrix(preds, testing_pca$classe)

#regularized discriminant analysis com pca

#set.seed(2104);
#modFit8 &lt;- train(classe ~ ., method=&quot;rda&quot;, data=training_pca);
#preds &lt;- predict(modFit8, newdata=testing_pca);
#confusionMatrix(preds, testing_pca$classe)

#-------------------------------------------------------------------------------------------------
#training for testing + training sample the best model, to evaluate with evaluation sample
#-------------------------------------------------------------------------------------------------

training_eval &lt;- estimdata_valid[inTrain,];
percentagem_training_eval &lt;- dim(training_eval)[1] / dim(estimdata_valid)[1];
percentagem_training_eval
</code></pre>

<pre><code>## [1] 0.8001
</code></pre>

<pre><code class="r">preObj_eval &lt;- preProcess(training_eval[,-1], method = &quot;pca&quot;, thresh=0.95);
preObj_eval
</code></pre>

<pre><code>## 
## Call:
## preProcess.default(x = training_eval[, -1], method = &quot;pca&quot;, thresh = 0.95)
## 
## Created from 15699 samples and 55 variables
## Pre-processing: principal component signal extraction, scaled, centered 
## 
## PCA needed 28 components to capture 95 percent of the variance
</code></pre>

<pre><code class="r">training_eval_pca &lt;- data.frame(classe = training_eval$classe, predict(preObj_eval, training_eval[,-1]));
evaluating_pca &lt;- data.frame(classe = evaluating$classe, predict(preObj_eval, evaluating[,-1]));

#evaluating the out-of-sample performance for the chosen model

set.seed(2104);
modFit_eval &lt;- train(classe ~ ., method=&quot;gbm&quot;, data=training_eval_pca, verbose=FALSE);
preds &lt;- predict(modFit_eval, newdata=evaluating_pca);
confusionMatrix(preds, evaluating_pca$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 992  48  23  30  27
##          B  28 609  45  20  54
##          C  32  60 583  62  31
##          D  55  17  21 512  27
##          E   9  25  12  19 582
## 
## Overall Statistics
##                                         
##                Accuracy : 0.836         
##                  95% CI : (0.824, 0.847)
##     No Information Rate : 0.284         
##     P-Value [Acc &gt; NIR] : &lt; 2e-16       
##                                         
##                   Kappa : 0.792         
##  Mcnemar&#39;s Test P-Value : 2.43e-10      
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.889    0.802    0.852    0.796    0.807
## Specificity             0.954    0.954    0.943    0.963    0.980
## Pos Pred Value          0.886    0.806    0.759    0.810    0.900
## Neg Pred Value          0.956    0.953    0.968    0.960    0.958
## Prevalence              0.284    0.193    0.174    0.164    0.184
## Detection Rate          0.253    0.155    0.149    0.131    0.148
## Detection Prevalence    0.285    0.193    0.196    0.161    0.165
## Balanced Accuracy       0.922    0.878    0.898    0.880    0.893
</code></pre>

<pre><code class="r">#------------------------------------------------------------------------------------------------
#finally, training the final model with all the data to be used with the provided testing data
#------------------------------------------------------------------------------------------------

#preObj &lt;- preProcess(estimdata_valid[,-1], method = &quot;pca&quot;, thresh=0.95);
#preObj

#training_final_pca &lt;- data.frame(classe = estimdata_valid$classe, predict(preObj, estimdata_valid[,-1]));

#set.seed(2104);
#modFit_final &lt;- train(classe ~ ., method=&quot;gbm&quot;, data=training_final_pca, verbose=FALSE);

prediction_final_pca &lt;- predict(preObj_eval, testdata_valid);
preds &lt;- predict(modFit_eval, newdata=prediction_final_pca);

prediction_submitted_data &lt;- predict(modFit_eval, newdata=prediction_final_pca);
</code></pre>

</body>

</html>

